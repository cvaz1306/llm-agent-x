# Python Execution Sandbox

LLM Agent X includes an optional Dockerized sandbox for executing Python code generated by the agent. This provides a secure and isolated environment, preventing potentially harmful code from affecting your host system.

## Features

-   **Isolated Execution**: Python code runs inside a dedicated Docker container.
-   **File Uploads**: The agent can request to upload scripts or data files to the sandbox before execution.
-   **Cloudpickle Support**: The sandbox can load complex Python objects (like functions or classes) from `cloudpickle` files, making them available to the executed code.
-   **REST API**: The sandbox is controlled via a simple REST API, which the agent's `exec_python` tool uses internally.

## How it Works with Docker Compose

When you run the application using `docker-compose up`, the sandbox is automatically built and started as one of the services.

In the `docker-compose.yml` file, you will find a service named `sandbox`:

```yaml
services:
  sandbox:
    build:
      context: ./sandbox
    ports:
      - "5000:5000"
    # ... other configurations
```

This configuration tells Docker Compose to:
1.  Build a Docker image from the `./sandbox` directory.
2.  Run a container from that image.
3.  Map port 5000 of the container to port 5000 on your local machine, making the sandbox API available at `http://localhost:5000`.

**You do not need to build or run the sandbox container manually.**

## Enabling Python Execution for Agents

For an agent to be able to execute Python code, its toolset must include the `exec_python` tool.

#### Interactive Mode (Recommended)

When running the application via Docker Compose, the `worker` service must be configured to include the `exec_python` tool upon initialization. This is a developer-level configuration within the worker's entrypoint script. Once enabled, the `InteractiveDAGAgent` can decide to use the tool to fulfill a task.

#### Legacy CLI

When using the `llm-agent-x` command-line tool, you can enable the tool for a single run by using the `--enable-python-execution` flag:

```sh
llm-agent-x recursive "Your task..." --enable-python-execution
```

## Agent Prompting

For an agent to effectively use the `exec_python` tool, it must be prompted to understand the tool's parameters. The task description or user instructions should guide the agent on how to provide:

-   `code: str`: The Python code string to execute.
-   `files_to_upload: list[str]`: A list of local file paths to upload to the sandbox before execution.
-   `cloud_pickle_files_to_load: list[str]`: A list of `.pkl` filenames (which should also be in `files_to_upload`) to be loaded into the execution scope.

## Direct Tool Usage Example (for Developers)

To understand or test how the `exec_python` tool interacts with the sandbox, you can call it directly in a Python script. This is similar to how the agent would use it internally.

First, ensure the sandbox is running (via `docker-compose up`). Then, run a script like the following:

```python
import asyncio
from llm_agent_x.tools.exec_python import exec_python
import cloudpickle
import os

# This must match the URL exposed by Docker Compose
os.environ["PYTHON_SANDBOX_API_URL"] = "http://localhost:5000"

async def test_sandbox_execution():
    # 1. Create dummy files for the test
    script_content = """
print("--- Sandbox Script Start ---")
# Check for loaded pickle data
my_data = LOADED_PICKLES.get('data.pkl')
print(f"Loaded data from data.pkl: {my_data}")

# Read an uploaded text file
with open("/workspace/input_file.txt", "r") as f:
    file_content = f.read().strip()
print(f"Content from input_file.txt: {file_content}")
print("--- Sandbox Script End ---")
"""
    with open("test_script.py", "w") as f: f.write(script_content)
    with open("input_file.txt", "w") as f: f.write("Hello from an uploaded file.")
    
    pickled_data = {"key": "value", "number": 123}
    with open("data.pkl", "wb") as f: cloudpickle.dump(pickled_data, f)

    # 2. Define parameters for the tool call
    code_to_run_script = "with open('/workspace/test_script.py') as f: exec(f.read())"
    files_to_upload = ["test_script.py", "input_file.txt", "data.pkl"]
    pickles_to_load = ["data.pkl"] # This path is relative to the sandbox workspace

    # 3. Call the tool
    try:
        result = await exec_python(
            code=code_to_run_script,
            files_to_upload=files_to_upload,
            cloud_pickle_files_to_load=pickles_to_load
        )
        print("\nResult from exec_python tool:")
        print(result)
    except Exception as e:
        print(f"Error calling exec_python: {e}")
    finally:
        # 4. Clean up dummy files
        for f in files_to_upload: os.remove(f)

if __name__ == "__main__":
    asyncio.run(test_sandbox_execution())
```

**Expected Output:**
```
Result from exec_python tool:
{'stdout': "--- Sandbox Script Start ---\nLoaded data from data.pkl: {'key': 'value', 'number': 123}\nContent from input_file.txt: Hello from an uploaded file.\n--- Sandbox Script End ---\n", 'stderr': '', 'error': None}
```