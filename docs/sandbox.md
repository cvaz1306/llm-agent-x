# Python Execution Sandbox

LLM Agent X includes an optional Dockerized sandbox environment for executing Python code generated by the agent. This sandbox provides a secure and isolated environment, preventing potentially harmful code from affecting your local system. It also allows for pre-loading files and `cloudpickle` objects into the execution namespace.

## Features

-   **Isolated Execution**: Python code runs inside a Docker container, separate from your main system.
-   **File Uploads**: Scripts or data files can be uploaded directly to the sandbox's workspace before code execution. This is useful for providing context or data to the generated Python scripts.
-   **Cloudpickle Support**: The sandbox can load `.pkl` files (created using the `cloudpickle` library) into the Python execution scope. This allows complex Python objects, including functions and custom classes, to be available to the executed code.
-   **REST API**: The sandbox is controlled via a simple REST API, which the `exec_python` tool uses to send code and files for execution.

## Prerequisites

-   Docker installed and running on your system.

## Building the Sandbox Image

Before you can run the sandbox, you need to build the Docker image.

1.  Navigate to the root directory of the `llm_agent_x` repository (if you cloned it).
2.  Run the following command:

    ```sh
    docker build -t python-sandbox ./sandbox
    ```
    This command builds an image named `python-sandbox` using the `Dockerfile` located in the `sandbox` directory.

## Running the Sandbox Container

Once the image is built, you can run the sandbox container:

```sh
docker run -d -p 127.0.0.1:5000:5000 --rm python-sandbox
```

Let's break down this command:
-   `docker run`: The command to run a Docker container.
-   `-d`: Runs the container in detached mode (in the background).
-   `-p 127.0.0.1:5000:5000`: Maps port 5000 of your host machine to port 5000 of the container. The `127.0.0.1:` part ensures it's only accessible from your local machine. The sandbox API server inside the container listens on port 5000.
-   `--rm`: Automatically removes the container when it exits. This is useful for keeping your system clean.
-   `python-sandbox`: The name of the Docker image to use.

The sandbox API will now be accessible at `http://localhost:5000` (or `http://127.0.0.1:5000`).

## Configuration

The `exec_python` tool within LLM Agent X needs to know the URL of the sandbox API.

-   **Default URL**: `http://localhost:5000`
-   **Custom URL**: You can set the `PYTHON_SANDBOX_API_URL` environment variable if your sandbox is running on a different host or port. Add this to your `.env` file or set it in your shell environment:
    ```env
    PYTHON_SANDBOX_API_URL=http://your_custom_host:your_port
    ```
    This environment variable should be set for the environment running `llm-agent-x` itself, not for the Docker container.

## Using the Sandbox with LLM Agent X

To enable the agent to use the `exec_python` tool with the sandbox:

1.  **Ensure the Sandbox is Running**: Start the Docker container as described above.
2.  **Enable Python Execution in CLI**: When running `llm-agent-x` from the command line, use the `--enable-python-execution` flag:
    ```sh
    llm-agent-x "Your task that might require python code" --enable-python-execution
    ```
3.  **Enable Python Execution in API**: When using the Python API, ensure the `exec_python` tool is included in `RecursiveAgentOptions` and `allow_tools` is `True`. The `exec_python` tool will automatically use the sandbox if `use_docker_sandbox=True` is passed to it (which is the default behavior when the agent calls it based on LLM instructions).
    ```python
    from llm_agent_x.tools.exec_python import exec_python

    agent_options = RecursiveAgentOptions(
        # ... other options
        tools=[exec_python, ...], # Add other tools as needed
        tools_dict={"exec_python": exec_python, "exec": exec_python, ...},
        allow_tools=True,
    )
    # The agent's LLM must be prompted to use exec_python
    # and its parameters like use_docker_sandbox, files_to_upload, etc.
    ```

4.  **Agent Prompting**: The agent's underlying LLM must be prompted in a way that it understands:
    *   That it *can* use the `exec_python` tool.
    *   How to specify parameters for `exec_python`, such as:
        *   `code: str`: The Python code string to execute.
        *   `use_docker_sandbox: bool = True`: Whether to use the Docker sandbox.
        *   `files_to_upload: list[str] = []`: A list of local file paths to upload to the sandbox's `/workspace/` before execution.
        *   `cloud_pickle_files_to_load: list[str] = []`: A list of `.pkl` file paths (relative to the sandbox's `/workspace/`, meaning they should typically also be in `files_to_upload`) to be loaded using `cloudpickle` into the `LOADED_PICKLES` dictionary in the execution scope.

    This usually involves providing clear instructions and examples in the main task description or user instructions given to the agent.

## Direct `exec_python` Tool Usage Example (for testing or direct calls)

The following example shows how the `exec_python` tool can be called directly in Python, demonstrating its sandbox interaction. This is similar to how the agent would use it internally.

```python
import asyncio
from llm_agent_x.tools.exec_python import exec_python
import cloudpickle
import os

async def test_sandbox_execution():
    # Ensure PYTHON_SANDBOX_API_URL is set if not default,
    # and the sandbox container is running.

    # 1. Create dummy files for demonstration
    script_content = """
try:
    my_data_obj = LOADED_PICKLES.get('data.pkl')
    if my_data_obj:
        print(f"Loaded data from data.pkl: {my_data_obj}")
    else:
        print("data.pkl not found in LOADED_PICKLES.")

    with open("/workspace/input_file.txt", "r") as f:
        file_content = f.read()
    print(f"Content of input_file.txt: {file_content}")

    print("Hello from sandbox script!")
except Exception as e:
    print(f"Error in sandbox script: {e}")
"""
    with open("test_script.py", "w") as f:
        f.write(script_content)

    with open("input_file.txt", "w") as f:
        f.write("This is some input data from a file.")

    pickled_data = {"key": "value", "number": 123}
    with open("data.pkl", "wb") as f:
        cloudpickle.dump(pickled_data, f)

    # 2. Define parameters for exec_python
    # Code to execute the uploaded script within the sandbox
    code_to_run_script = "with open('/workspace/test_script.py', 'r') as f: exec(f.read())"

    files_to_upload_to_sandbox = ["test_script.py", "input_file.txt", "data.pkl"]

    # These paths are relative to the sandbox's /workspace/ directory
    cloud_pickle_files_in_sandbox = ["data.pkl"]

    # 3. Execute using the sandbox
    try:
        result = await exec_python(
            code=code_to_run_script,
            use_docker_sandbox=True,
            files_to_upload=files_to_upload_to_sandbox,
            cloud_pickle_files_to_load=cloud_pickle_files_in_sandbox
        )
        print("\nResult from exec_python:")
        print(result)
    except Exception as e:
        print(f"Error calling exec_python: {e}")
    finally:
        # 4. Clean up dummy files
        os.remove("test_script.py")
        os.remove("input_file.txt")
        os.remove("data.pkl")

if __name__ == "__main__":
    # Make sure your sandbox Docker container is running before executing this.
    # You might need to set PYTHON_SANDBOX_API_URL if it's not http://localhost:5000
    # Example: os.environ["PYTHON_SANDBOX_API_URL"] = "http://127.0.0.1:5000"
    asyncio.run(test_sandbox_execution())
```

**Expected Output (or similar):**
```
Result from exec_python:
{'stdout': "Loaded data from data.pkl: {'key': 'value', 'number': 123}\nContent of input_file.txt: This is some input data from a file.\nHello from sandbox script!\n", 'stderr': '', 'error': None}
```

This sandbox significantly enhances the safety and capability of the LLM Agent X when dealing with Python code execution tasks. Remember to manage the Docker container lifecycle as needed for your use case.
